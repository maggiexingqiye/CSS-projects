{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f829b8c",
   "metadata": {},
   "source": [
    "**Large Language Model and Text Generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073da65",
   "metadata": {},
   "source": [
    "Use GPT-2 or any other Large Language Model (LLM) with the Hugging Face library to generate few words using input text for your choice. In the lab, we generated text using input text: “I study sociology at University of California.” Use suitable input text and generate very short text to describe this project. We understand that these models are not perfect and we are not expecting the generated text to be perfectly coherent or sensible.\n",
    "If time permits, you are encouraged to tune hyperparameters but are not explicitly required to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22854f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ba7600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5d3dc2d2e54a69ac70d09c239ddf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd5192bb937435bba3e06f1abe2db4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556f8f7f824a4467b0669d271632e361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ab1140e8f94babb545a7cb6cde4a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a1c0b9508345da8579fc215f760527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress, Senate and House members and other members.\n",
      "\n",
      "Because each tweet is encrypted, all text datasets are encrypted at the time of publication. Therefore, if you copy\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress (see http://spf.usna.gov/spfnews.cgi?), and then to apply them to an entire dataset of tweets by various political party\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress, including current and past President Barack Obama. They then conducted a series of regressions to show if there was a significant difference between the datasets. Using both regression and\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. The data set (based upon US Census data) is then analyzed at the congressional level. As an additional sample, analyses are also made to include data from an\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. One of U.S. government's key metrics, the \"ratings system,\" uses Twitter to predict how much one should rate a comment or comment by its\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "input_text = \"use NLP methods to analyze a text dataset containing tweets from members of United States Congress\"\n",
    "output = generator(input_text, max_length=50, num_return_sequences=5)\n",
    "for result in output:\n",
    "    print(result['generated_text'])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e359a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. In fact, a large portion of our data came from users' tweets and had to be analyzed. In the past, we have used NLP methods to analyze a text dataset (see Text Structure Search [textdata.gzip]) that was then compared against\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. (A Twitter user created a blog post on the topic on November 13 of 2013 titled \"Why Twitter Is Great.\") A tweet in December 2012 by one of these Twitter users provided a link to the content of a subsequent tweet suggesting that President Obama had tweeted something about\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. The data were then gathered and analyzed using Google Translate (http://www.google.com/translate/en/downloads/) and Google Scholar. The analyses involved several different methods, including Google Translate (http://www.google.com/\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress, members of Congress and the press.\n",
      "\n",
      "The study used a special type of method called an Internet Search Query (IQQ) method to identify information in a text dataset that is highly classified. It is designed to identify topics that are not covered by classified\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. These included all national and Congressional figures, tweets that were published in response to a request from a single tweet, and tweets that were published for news releases.\n",
      "\n",
      "The authors found that the public and media responded negatively when Twitter responded negatively to tweets from congressional members\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The data were collected from the National Center for Social and Economic Data (NCES) and the National Center for Health Statistics (NCHES). The NCES data were collected from the National Center for Health Statistics (NCHES) and the National\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The data were collected from the National Center for Social and Economic Data (NCES) and the National Center for Health Statistics (NCHS). The NCES data were collected from the National Center for Health Statistics (NCHS) and the National Center for\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The study was funded by the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation,\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The researchers used a dataset of tweets from members of the House of Representatives and the Senate, and a dataset of tweets from members of the Senate and the House of Representatives. The researchers then used the data to analyze the tweets and then used the data to\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. The dataset contains a total of 1,919 tweets from members of Congress. The dataset contains a total of 1,919 tweets from members of Congress.\n",
      "\n",
      "The dataset contains a total of 1,919 tweets from members of Congress. The dataset\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The data were collected from the National Center for Social and Economic Data (NCES) and the National Center for Health Statistics (NCHES). The NCES data were collected from the National Center for Health Statistics (NCHES) and the National\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The data were collected from the National Center for Social and Economic Data (NCES) and the National Center for Health Statistics (NCHS). The NCES data were collected from the National Center for Health Statistics (NCHS) and the National Center for\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The study was funded by the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation,\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress.\n",
      "\n",
      "The researchers used a dataset of tweets from members of the House of Representatives and the Senate, and a dataset of tweets from members of the Senate and the House of Representatives. The researchers then used the data to analyze the tweets and then used the data to\n",
      "----\n",
      "use NLP methods to analyze a text dataset containing tweets from members of United States Congress. The dataset contains a total of 1,919 tweets from members of Congress. The dataset contains a total of 1,919 tweets from members of Congress.\n",
      "\n",
      "The dataset contains a total of 1,919 tweets from members of Congress. The dataset\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# experiment changing different hyper parameters in the model above\n",
    "output = generator(input_text, max_length=70, num_return_sequences=5, temperature = 0.9)\n",
    "for result in output:\n",
    "    print(result['generated_text'])\n",
    "    print(\"----\")\n",
    "    \n",
    "output2 = generator(input_text, max_length=70, num_return_sequences=5, temperature = 0.1)\n",
    "for result in output2:\n",
    "    print(result['generated_text'])\n",
    "    print(\"----\")\n",
    "    \n",
    "output3 = generator(input_text, max_length=100, num_return_sequences=5, temperature = 0.5)\n",
    "for result in output2:\n",
    "    print(result['generated_text'])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3954c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
